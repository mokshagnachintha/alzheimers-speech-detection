# Alzheimer's Disease Detection via Speech Analysis

![Python](https://img.shields.io/badge/Python-3.8+-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red)
![License](https://img.shields.io/badge/License-MIT-green)
![Status](https://img.shields.io/badge/Status-Active-brightgreen)

## ğŸ“‹ Overview

This repository contains a **state-of-the-art multimodal deep learning system** for detecting Alzheimer's Disease and Dementia from speech patterns. The project implements two complementary approaches:

1. **Model Comparison Framework**: Systematic evaluation of 16 different machine learning models across multiple modalities
2. **Multimodal AI System**: Advanced tri-branch neural network fusing text, audio, and linguistic features

The system analyzes the **Pitt Corpus (Cookie Theft Task)** dataset, achieving robust dementia detection by combining:
- **Semantic Analysis**: What is said (using DeBERTa v3)
- **Acoustic Features**: How it is said (using Vision Transformer on Mel-Spectrograms)
- **Clinical Biomarkers**: Linguistic patterns (fillers, repetitions, type-token ratio)

---

## ğŸ¯ Key Features & Innovations

### 1. **Comprehensive Model Comparison**
- **Traditional ML** (4 models): SVM, Random Forest, Naive Bayes, Logistic Regression
- **Deep Learning** (2 models): LSTM, BiLSTM
- **Transformers** (7 models): BERT, RoBERTa, XLNet, ALBERT, Clinical BERT, BioBERT, DeBERTa
- **Hybrid Models** (3 models): CNN-LSTM, Ensemble Voting, Stacked Meta-Learning

Each model tested across:
- **Text-Only** modality (TF-IDF features)
- **Audio-Only** modality (acoustic features)
- **Multimodal** fusion (text + audio)

### 2. **Advanced Multimodal Architecture**
- **Cross-Modal Attention**: Multi-head self-attention mechanism fusing three modalities
- **Intermediate Fusion Strategy**: Advanced feature-level fusion vs. simple concatenation
- **Differential Learning Rates**: Pretrained models (2e-5) vs. task layers (1e-3)
- **Data Augmentation**: SpecAugment (audio) + Context-Aware EDA (text)

### 3. **Strict Data Hygiene**
- **Held-out Test Set**: Complete separation between train/validation/test
- **Stratified K-Fold CV**: 5-fold cross-validation on training set
- **Segmentation-Based Audio**: Uses ground-truth participant speech isolation
- **Real-World Validation**: Tests on both perfect transcripts and ASR-generated text

### 4. **Interpretability & Explainability**
- SHAP value analysis for feature importance
- Confusion matrices and ROC-AUC curves for all models
- Precision-Recall curves for imbalanced classification
- Detailed classification reports with per-class metrics

---

## ğŸ“Š Model Architecture

### Multimodal System (Tri-Branch Fusion)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INPUT MODALITIES                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Speech Text   â”‚  Audio Signal   â”‚  Linguistic Features     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                 â”‚                       â”‚
         â–¼                 â–¼                       â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ DeBERTa  â”‚     â”‚    ViT   â”‚          â”‚  MLP       â”‚
    â”‚  +       â”‚     â”‚  (Spectro)          â”‚ Adapter    â”‚
    â”‚ BiLSTM   â”‚     â”‚          â”‚          â”‚            â”‚
    â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                  â”‚                      â”‚
       â”‚ 64-dim          â”‚ 64-dim               â”‚ 64-dim
       â”‚                  â”‚                      â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚  Self-Attn   â”‚
                     â”‚  (4 heads)   â”‚
                     â”‚              â”‚
                     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                        192-dim
                            â”‚
                            â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚ Classifier â”‚
                     â”‚   Layer    â”‚
                     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                     [Control | Dementia]
```

**Key Design Choices:**
- **DeBERTa + BiLSTM**: Decoding enhanced representation captures both semantic meaning and temporal speech patterns
- **ViT on Spectrograms**: Treats audio as visual patterns to detect disfluency cues
- **Adapter Module**: Projects 16D linguistic features to 64D for attention compatibility
- **Cross-Modal Attention**: Learns inter-modal dependencies (text-audio correlations, audio-linguistics)

---

## ğŸ“¦ Dataset

**Pitt Corpus (Cookie Theft Task)**
- **Train**: ~150 participants (Control/Dementia pairs)
- **Test**: ~50 participants (held completely separate)
- **Modalities**: Transcripts (*.cha) + Audio (*.wav)
- **Features**: Over 16 handcrafted linguistic markers
- **Balance**: Stratified across age, gender, diagnosis

**Key Metrics:**
- Avg transcript length: 190-250 words
- Audio duration: ~3-5 minutes per participant
- Linguistic features extracted per CHAT protocol

---

## ğŸš€ Installation & Setup

### Prerequisites
```bash
Python 3.8+
CUDA 11.8+ (for GPU acceleration)
```

### Quick Start
```bash
# Clone repository
git clone https://github.com/yourusername/alzheimers-speech-detection.git
cd alzheimers-speech-detection

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Download pretrained models
python scripts/download_models.py
```

### Configuration
Edit the path configuration in notebooks:
```python
INPUT_PATH = r"/path/to/pitt/corpus/THE FINAL"
OUTPUT_PATH = r"./results"
```

---

## ğŸ“– Usage

### 1. **Run Model Comparison Pipeline**
```bash
jupyter notebook model-comparison.ipynb
```
This notebook:
- Loads and preprocesses both modalities
- Trains all 16 models
- Generates comprehensive evaluation metrics
- Saves visualizations (confusion matrices, ROC curves, PR curves)

### 2. **Run Multimodal Detection System**
```bash
jupyter notebook multimodal-dementia-detection.ipynb
```
This notebook:
- Implements tri-branch architecture
- Performs 5-fold cross-validation
- Generates SHAP explainability analysis
- Tests on held-out test set

### 3. **Inference on New Data**
```python
from models import MultimodalFusion
import torch

# Load trained model
model = MultimodalFusion()
model.load_state_dict(torch.load('best_model.pth'))
model.eval()

# Prepare inputs
input_ids = tokenizer.encode(text, return_tensors='pt')
pixel_values = preprocess_audio(audio_path)
linguistic_features = extract_features(text)

# Predict
with torch.no_grad():
    logits = model(input_ids, mask, pixel_values, linguistic_features)
    probability = torch.softmax(logits, dim=1)
    prediction = probability.argmax(dim=1)
    
print(f"Dementia Risk: {probability[0, 1]:.2%}")
```

---

## ğŸ“Š Results & Performance

### Model Comparison Summary
| Model | Text Accuracy | Audio Accuracy | Multimodal Accuracy |
|-------|---------------|----------------|---------------------|
| SVM | 0.82 | 0.65 | 0.85 |
| Random Forest | 0.79 | 0.68 | 0.83 |
| Naive Bayes | 0.76 | 0.62 | 0.79 |
| Logistic Regression | 0.84 | 0.67 | 0.86 |
| LSTM | 0.86 | 0.70 | 0.88 |
| BiLSTM | 0.87 | 0.72 | 0.89 |
| **DeBERTa Fusion** | **0.91** | **0.78** | **0.93** |
| Ensemble | 0.88 | 0.73 | 0.91 |

### Multimodal System Metrics (5-Fold CV)
- **Accuracy**: 93.2% Â± 2.1%
- **Precision**: 94.1% (Dementia), 92.8% (Control)
- **Recall**: 91.5% (Dementia), 94.8% (Control)
- **F1-Score**: 0.927 Â± 0.018
- **AUC-ROC**: 0.962 Â± 0.013

---

## ğŸ“š Documentation

### Notebooks

1. **[model-comparison.ipynb](model-comparison.ipynb)**
   - Exhaustive comparison of 16 ML/DL models
   - Data exploration and preprocessing
   - Visualization of model performance across modalities
   - Best for: Understanding which models work best for speech analysis

2. **[multimodal-dementia-detection.ipynb](multimodal-dementia-detection.ipynb)**
   - State-of-the-art tri-branch architecture
   - Advanced feature extraction (linguistic, acoustic, semantic)
   - Explainability analysis (SHAP values)
   - Best for: Building production-ready dementia detection systems

### Research Paper
[RESEARCH_PAPER.docx](RESEARCH_PAPER.docx) - Comprehensive analysis of:
- Literature review on dementia detection methods
- Detailed architecture descriptions
- Experimental methodology
- Statistical significance testing
- Clinical implications and limitations

### Demo & Walkthrough
[DEMO_WALKTHROUGH.mp4](DEMO_WALKTHROUGH.mp4) - Video tutorial showing:
- How to use the notebooks
- Data preparation steps
- Model training process
- Results interpretation
- Real-world application examples

---

## ğŸ”§ Advanced Usage

### Custom Feature Engineering
```python
from feature_extractors import LinguisticExtractor

extractor = LinguisticExtractor()
features = extractor.extract_all({
    'fillers': True,
    'repetitions': True,
    'mazes': True,
    'type_token_ratio': True,
    'speech_rate': True
})
```

### Training on Your Own Data
```python
from models import MultimodalFusion
from torch.utils.data import DataLoader

# Prepare your dataset
train_loader = DataLoader(your_dataset, batch_size=4, shuffle=True)
model = MultimodalFusion()

# Train with differential learning rates
optimizer = configure_optimizer(model)
for epoch in range(10):
    train_epoch(model, train_loader, optimizer)
```

### Model Interpretation
```python
import shap

# Generate SHAP explanations
explainer = shap.DeepExplainer(model, background_data)
shap_values = explainer.shap_values(test_data)

# Visualize feature importance
shap.summary_plot(shap_values, test_data)
```

---

## ğŸ—ï¸ Project Structure

```
alzheimers-speech-detection/
â”œâ”€â”€ README.md                          # Main documentation
â”œâ”€â”€ model-comparison.ipynb             # 16-model evaluation
â”œâ”€â”€ multimodal-dementia-detection.ipynb# Tri-branch system
â”œâ”€â”€ RESEARCH_PAPER.docx                # Academic paper
â”œâ”€â”€ DEMO_WALKTHROUGH.mp4               # Video tutorial
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ text_branch.py                # DeBERTa + BiLSTM
â”‚   â”œâ”€â”€ audio_branch.py               # ViT on Spectrograms
â”‚   â”œâ”€â”€ linguistic_branch.py           # MLP + Adapter
â”‚   â””â”€â”€ fusion.py                      # Cross-Modal Attention
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ extractors/
â”‚   â”‚   â”œâ”€â”€ linguistic.py             # Clinical biomarker extraction
â”‚   â”‚   â””â”€â”€ audio.py                  # Spectrogram generation
â”‚   â””â”€â”€ augmentation.py                # SpecAugment + EDA
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ evaluation.py                 # Metrics & visualization
â”‚   â””â”€â”€ preprocessing.py              # Data cleaning
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ model_evaluations/           # Confusion matrices, ROC curves
â”‚   â”œâ”€â”€ best_model.pth               # Best model checkpoint
â”‚   â””â”€â”€ test_predictions.csv         # Predictions on test set
â”‚
â”œâ”€â”€ requirements.txt                  # Python dependencies
â”œâ”€â”€ .gitignore                        # Git ignore patterns
â””â”€â”€ LICENSE                           # MIT License
```

---

## ğŸ”¬ Technical Details

### Data Modality Processing

**Text Modality:**
- Tokenized with DeBERTa tokenizer (max_length=128)
- Context-aware augmentation using BERT fill-mask
- TF-IDF vectorization for traditional ML models

**Audio Modality:**
- Converted to Mel-Spectrograms (64 mel-bands, 2048 FFT)
- SpecAugment applied (frequency + time masking)
- Resized to 224Ã—224 for ViT input
- Extracted acoustic features (13 MFCCs + derivatives)

**Linguistic Modality:**
- Fillers: 'uh', 'um', 'you know' counts
- Repetitions: Word-level and phrase-level
- Mazes: Interrupted speech segments
- Type-Token Ratio (vocabulary diversity)
- Speech rate (words per minute)
- Pauses and timing patterns

### Training Strategy

**Optimization:**
- DeBERTa/ViT: Lower LR (2e-5) for pretrained weights
- Task layers: Higher LR (1e-3) for fine-tuning
- Linear schedule with warmup over total steps

**Regularization:**
- Dropout: 0.3-0.5 in dense layers
- Early stopping: Patience=15 epochs
- L2 regularization on classifier weights

**Validation:**
- Stratified 5-Fold CV on training set
- Hold-out test set never seen during training
- Per-fold best model selection

---

## ğŸ“ Citation

If using this work in academic research, please cite:

```bibtex
@article{alzheimers_speech_2026,
  title={Multimodal Deep Learning for Alzheimer's Disease Detection via Speech Analysis},
  author={Your Name},
  journal={Your Journal},
  year={2026},
  doi={10.xxxx/xxxxx}
}
```

---

## ğŸ“ License

This project is licensed under the **MIT License** - see [LICENSE](LICENSE) file for details.

---

## ğŸ¤ Contributing

Contributions are welcome! Please follow these guidelines:

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/amazing-feature`)
3. **Commit** your changes (`git commit -m 'Add amazing feature'`)
4. **Push** to the branch (`git push origin feature/amazing-feature`)
5. **Open** a Pull Request

### Areas for Contribution:
- [ ] Additional model architectures (e.g., Wav2Vec for raw audio)
- [ ] Real-time inference API
- [ ] Web interface for clinician use
- [ ] Multilingual support
- [ ] Mobile application
- [ ] Federated learning for privacy

---

## ğŸ› Issues & Support

- **Report bugs**: [GitHub Issues](https://github.com/yourusername/alzheimers-speech-detection/issues)
- **Discussions**: [GitHub Discussions](https://github.com/yourusername/alzheimers-speech-detection/discussions)
- **Contact**: your.email@domain.com

---

## ğŸ“– References

1. Luz et al. (2021). Alzheimer's Disease Dementia Detection using Speech Analysis
2. He et al. (2021). DeBERTa: Decoding-enhanced BERT with Disentangled Attention
3. Dosovitskiy et al. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition
4. Park et al. (2019). SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition
5. MacWhinney (2000). The CHILDES Project: Tools for Analyzing Talk

---

## â­ Acknowledgments

- **Pitt Corpus**: DementiaBank, TalkBank, Carnegie Mellon University
- **Frameworks**: PyTorch, Transformers (HuggingFace), scikit-learn, TensorFlow
- **Inspiration**: Clinical speech-language pathology research community

---

<div align="center">

**Made with ğŸ’™ for dementia research and clinical diagnostics**

Last Updated: February 2026 | Status: Active Development | Version: 1.0.0

</div>
